#!/usr/bin/python

'''
Calculates the GC bias curve of an Illumina sequencing run.

Takes three inputs: a BAM file of aligned reads, the reference FASTA file that the BAM file was aligned to, and the mapability
bed file of the reference FASTA file (generated by find_unique_regions.py).

The BAM file should be from a single library.

Changes from v1:
Updated processing of mapability files to reflect format change of mapability files 
from .mapabiity to .bed
Added smoothing of extreme outliers

Changes from v2:
Estimates fragment size distribution from actual distribution in sample, without 
assumption of normality

Changes from v3:
Gets approximately even numbers of each GC composition, rather than reflecting
the distribution within the reference genome
Fixed off-by-one error in Step 2 introduced by change from mapability format to bed format
Accounted for possibility of 100% GC fragments
Changed normalization process to normalize by expected coverage from total read pairs in file
'''

import time
import math
import numpy
import pysam
import random
import argparse
import subprocess
import get_bam_info
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
from Bio import SeqIO
from scipy import stats
from Bio.Statistics import lowess


def gc_content(seq, seqlen):
	return float(seq.count('G') + seq.count('C') + seq.count('g') + seq.count('c')) / seqlen

def build_hist_dist(hist):
	binsize = hist[1][1] - hist[1][0]
	probs = []
	fragsizes = []
	for prob, fsize in zip(hist[0],hist[1]):
		for size in xrange(binsize):
			fragsizes.append(fsize+size)
			probs.append(prob)
	tot = float(sum(probs))
	min_frag = fragsizes[min([idx for idx, prob in enumerate(probs) if prob > 0])]
	max_frag = fragsizes[max([idx for idx, prob in enumerate(probs) if prob > 0])]
	probs = numpy.cumsum([x/tot for x in probs])
	return probs, fragsizes, min_frag, max_frag

def read_gc_file(filename):
	infile = open(filename, 'r')
	gc_curve = [[],[],[]]
	for line in infile:
		if not line.startswith('#'):
			data = line.split()
			gc_curve[0].append(float(data[1]))
			gc_curve[1].append(float(data[3]))
			gc_curve[2].append(float(data[4]))
	return gc_curve

if __name__ == '__main__':

	parser = argparse.ArgumentParser(description = "Calculates the GC bias curve of an Illumina sequencing run")
	parser.add_argument("alignment", help = "BAM file of aligned Illumina reads")
	parser.add_argument("reference", help = "FASTA file of reference genome that the BAM file was aligned to")
	parser.add_argument("mapfile", help = "Mapability and GC content bed file of reference genome for the fragment length of the BAM file")
	parser.add_argument("-l", "--locations", help = "Number of random mappable locations in the genome to sample (default = 10 million)", type = int, default = 10000000)
	parser.add_argument("-b", "--bins", help = "Number of GC content bins (default = 200)", type = int, default = 200)
	parser.add_argument("-o", "--output", help = "Output filename of GC curve (default: input BAM filename + .gc_curve)")
	parser.add_argument("-p", "--plot", help = "Plot the GC curve and save plot as output filename + .png", action="store_true")

	args = parser.parse_args()

	alignment_bam = pysam.Samfile(args.alignment, 'rb')
	mapfile = open(args.mapfile, 'r')
	reference = SeqIO.index(args.reference, 'fasta')
	
	
	#Step 1: Get BAM information and build distribution of fragment sizes
	print 'Getting BAM information...'

	frag_lens, read_lens = get_bam_info.get_bam_info(alignment_bam)
	read_len = int(stats.mode(read_lens)[0])
	frag_hist = get_bam_info.get_frag_hist(frag_lens)
	frag_probs, frag_sizes, min_frag, max_frag = build_hist_dist(frag_hist)
	
	
	#Step 2: Generate random locations and build array of locations in each GC content window:
	print 'Generating random locations in genome...'

	genome_gc_windows = []
	for line in mapfile:
		if line.startswith('#GC Content of'):
			genome_gc_windows.append(int(line.split()[4]))
		elif not line.startswith('#'):
			break
	mapfile.seek(0)
	
	win_count = len(genome_gc_windows)

	selected_gc_windows = [0] * win_count

	while sum(selected_gc_windows) < args.locations:
		selected_gc_windows = [x + (args.locations - sum(selected_gc_windows))/win_count for x in selected_gc_windows]
		selected_gc_windows = [min(selected_gc_windows[x], genome_gc_windows[x]) for x in range(win_count)]
		if args.locations - sum(selected_gc_windows) < win_count:
			for leftover in xrange(args.locations - sum(selected_gc_windows)):
				selected_gc_windows[random.randrange(win_count)] += 1
			selected_gc_windows = [min(selected_gc_windows[x], genome_gc_windows[x]) for x in range(win_count)]
		if selected_gc_windows == genome_gc_windows:
			print 'Number of locations chosen is higher than number of mappable locations in genome; choosing all locations in genome.'
			break
	
	gc_location_counts = [0] * (args.bins + 1) #Array of locations in each GC content window; +1 accounts for 100% GC fragments

	sample_locations_indeces = [set([]) for x in selected_gc_windows]
	for window in xrange(win_count):
		while len(sample_locations_indeces[window]) < selected_gc_windows[window]:
			sample_locations_indeces[window].add(random.randrange(genome_gc_windows[window]))
		
	map_index = [0] * win_count
	sample_locations = {}
	current_chrom = None
	for line in mapfile:
		if line.startswith('#'):
			continue
		if line.split()[0] != current_chrom:
			current_chrom = line.split()[0]
			reference_chrom = reference[current_chrom]
			if current_chrom not in sample_locations:
				sample_locations[current_chrom] = set([])
		window = int(float(line.split()[3]) * win_count)
		for base in xrange(int(line.split()[1]), int(line.split()[2])):
			if map_index[window] in sample_locations_indeces[window]:
				sample_locations[current_chrom].add(base + 1) #The +1 is because SAM/BAM files are 1-indexed			
				size = frag_sizes[numpy.searchsorted(frag_probs, random.random())]
				gc_location_counts[int(gc_content(reference_chrom.seq[base: base + size], size) * args.bins)] += 1
			map_index[window] += 1
	
	
	#Step 3: Iterate through BAM file and build GC curve
	print 'Building GC curve...'

	gc_read_counts = [0] * (args.bins + 1) #Array of actual reads in each GC content window

	alignment = subprocess.Popen('samtools view %s' %(args.alignment), shell=True, stdout=subprocess.PIPE, bufsize = 1)
	current_chrom = None
	for read in alignment.stdout:
		data = read.split()
		if data[2] != current_chrom:
			if data[2] not in sample_locations.keys():
				continue
			current_chrom = data[2]
			reference_chrom = reference[current_chrom]
		if int(data[3]) in sample_locations[data[2]]:
			if int(data[1]) & 2 == 2 and int(data[4]) > 30 and (data[5] == '%iM' %(read_len) or (data[5].endswith('M') and data[5][:-1].isdigit() and int(data[5][:-1]) > read_len)):
				if max_frag >= int(data[8]) >= min_frag:
					frag_gc = gc_content(reference_chrom.seq[int(data[3]) - 1:int(data[3]) + int(data[8]) - 1], int(data[8]))
					gc_read_counts[int(frag_gc * args.bins)] += 1
		

	#Normalize to mean coverage in the entire file
	read_norm, loc_norm = (alignment_bam.mapped + alignment_bam.unmapped)/2, sum(alignment_bam.lengths)
	gc_curve = [0.0 if loc_count == 0 else float(read_count * loc_norm)/(loc_count * read_norm) for read_count, loc_count in zip(gc_read_counts, gc_location_counts)]
	
	#Correct any extreme outliers caused by low read count
	outliers = []
	for window in xrange(len(gc_curve)):
		if gc_read_counts[window] < 10:
			if window == 0 and gc_curve[window] - 0.5 > gc_curve[window + 1]:
				outliers.append(window)
				gc_curve[window] = gc_curve[window + 1]
			elif window == len(gc_curve) - 1 and gc_curve[window] - 0.5 > gc_curve[window - 1]:
				outliers.append(window)
				gc_curve[window] = gc_curve[window + 1]
			elif gc_curve[window] - 0.5 > gc_curve[window - 1] and gc_curve[window] - 0.5 > gc_curve[window + 1]:
				outliers.append(window)
				gc_curve[window] = (gc_curve[window - 1] + gc_curve[window + 1])/2.
	
	gc_curve = numpy.array(gc_curve)
	gc_x = numpy.array([x / float(args.bins) for x in xrange(args.bins + 1)])
	smoothed_gc_curve = lowess.lowess(gc_x, gc_curve, f = 0.1, iter = 1)
	smoothed_gc_curve = [max(0.0, x) for x in smoothed_gc_curve]

	if args.output:
		outname = args.output
	else:
		if args.alignment.endswith('bam') and '.' in args.alignment:
			outname = '%s.gc_curve' %('.'.join(args.alignment.split('.')[:-1]))
		else:
			outname = '%s.gc_curve' %(args.alignment)

	outfile = open(outname, 'w')

	outfile.write('# GC Curve file of %s generated by %s on %s at %s\n' %(args.alignment, __file__.split('/')[-1], time.strftime('%m/%d/%Y'), time.strftime('%H:%M:%S')))
	outfile.write('# Curve calculated from %i reads at %i locations\n' %(sum(gc_read_counts), sum(selected_gc_windows)))
	if len(outliers) > 0:
		outfile.write('# Windows with corrected extreme outlier GC bias: %s\n' %(', '.join(['%.*f-%.*f' %(len(str(args.bins)) + 2, window * 1.0/args.bins, len(str(args.bins)) + 2, min(1., (window + 1) * 1.0/args.bins - (1. / 10 ** (len(str(args.bins)) + 2)))) for window in outliers])))
	outfile.write('#\n')
	outfile.write('#GC_content\tSmoothed_GC_bias\tRaw_GC_Bias\tNo_of_reads\tNo_of_locations\n')
	for window, bias in enumerate(gc_curve):
		outfile.write('%.*f-%.*f\t%f\t%f\t%i\t%i\n' %(len(str(args.bins)) + 2, window * 1.0/args.bins, len(str(args.bins)) + 2, min(1., (window + 1) * 1.0/args.bins - (1. / 10 ** (len(str(args.bins)) + 2))), smoothed_gc_curve[window], bias, gc_read_counts[window], gc_location_counts[window]))

	if args.plot:
		plt.clf()
		plt.plot([x/float(args.bins) for x in range(args.bins + 1)], gc_curve, 'o')
		plt.plot([x/float(args.bins) for x in range(args.bins + 1)], smoothed_gc_curve, 'red')
		plt.xlabel('GC content')
		plt.ylabel('GC bias')
		plt.title('GC Bias in %s' %(args.alignment.split('/')[0]))
		plt.savefig(outname + '.png')

	outfile.close()
	alignment_bam.close()
	mapfile.close()
	reference.close()